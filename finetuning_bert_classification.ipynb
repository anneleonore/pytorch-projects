{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I. Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange  #for progress bars\n",
    "\n",
    "#from IPython.display import Image #for image rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II. Specify CUDA as the device for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Display information about the NVIDIA GPU drivers and hardware\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### III. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  428k  100  428k    0     0   699k      0 --:--:-- --:--:-- --:--:--  701k\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-and-Computer-Vision-3rd-Edition/master/Chapter05/in_domain_train.tsv --output \"in_domain_train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_excel('C:/Users/JackCarey/Documents/Noor/FILENAME.xlsx')\n",
    "df = pd.read_csv(\"in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8551, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>cj99</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i 'm not shocked by the idea that the more you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dried the clothes in the sun .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>which rebel leader did you hear cheney 's rumo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157</th>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bill wants john to leave .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we saw him beaten by the champion .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6763</th>\n",
       "      <td>m_02</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>because into the room came aunt norris , fanny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7247</th>\n",
       "      <td>sks13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>john knows that she left and john knows whethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nora pushed her way through the crowd .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tom locked fido in the garage .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>edward 's help , you can rely on .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "108             cj99      1         NaN   \n",
       "2393            l-93      1         NaN   \n",
       "4973            ks08      0           *   \n",
       "6157            c_13      1         NaN   \n",
       "3971            ks08      1         NaN   \n",
       "6763            m_02      0           *   \n",
       "7247           sks13      1         NaN   \n",
       "2716            l-93      1         NaN   \n",
       "3957            ks08      1         NaN   \n",
       "4759            ks08      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "108   i 'm not shocked by the idea that the more you...  \n",
       "2393                   i dried the clothes in the sun .  \n",
       "4973  which rebel leader did you hear cheney 's rumo...  \n",
       "6157                         bill wants john to leave .  \n",
       "3971                we saw him beaten by the champion .  \n",
       "6763  because into the room came aunt norris , fanny...  \n",
       "7247  john knows that she left and john knows whethe...  \n",
       "2716            nora pushed her way through the crowd .  \n",
       "3957                    tom locked fido in the garage .  \n",
       "4759                 edward 's help , you can rely on .  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print sample\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IV. Creating sentences, label lists, and adding BERT tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Pandas we access a column in a DataFrame using the dot notation (df.column_name) or the bracket notation (df['column_name'])\n",
    "sentences = df.sentence.values\n",
    "#print(repr(sentences)) #NumPy array of strings including the comma's\n",
    "\n",
    "#Adding CLS and SEP tokens at the beginning and end of each sentence for BERT\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "#print(sentences)\n",
    "\n",
    "# Extracting the labels from the DataFrame\n",
    "labels = df.label.values\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### V. Creating an instance of the BertTokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    print(\"Tokenizer downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while downloading the tokenizer.\")\n",
    "    print(str(e)) #Extract the error message\n",
    "    import traceback\n",
    "    print(traceback.format_exc()) #Gives more context about the error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'our', 'friends', 'wo', 'n', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#Use the tokenize() method to tokenize each sentence in sentences\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VI. Set max_len, convert tokens to IDs (index numbers in BERT vocabulary), and pad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
    "#In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128\n",
    "\n",
    "#Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "#print(input_ids)\n",
    "\n",
    "#Pad the input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "#print(input_ids) #Remember: the special token <PAD> is represented by 0 in the BERT vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VII. Create attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "\n",
    "#Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq] #Conditional statement: float(TRUE) > 1.0\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIII. Split data into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and validation inputs and labels\n",
    "#10% of the data (input_ids) is used for validation_inputs\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                                                    random_state=123, test_size=0.1)\n",
    "#Training and validation masks\n",
    "#We are only focusing on the first two arrays (train_masks and validation_masks), which are splits of attention_masks\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=123, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IX. Converting data into torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus: Training dataset\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "#Focus: Validation dataset\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X. Select batch size and create iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader (train_dataloader):\n",
      "[tensor([[  101, 13097, 26393,  ...,     0,     0,     0],\n",
      "        [  101,  2009,  2001,  ...,     0,     0,     0],\n",
      "        [  101,  2198,  2387,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2198,  5720,  ...,     0,     0,     0],\n",
      "        [  101,  2585,  8823,  ...,     0,     0,     0],\n",
      "        [  101,  1037,  3899,  ...,     0,     0,     0]]), tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]]), tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "#Select a batch size for training (recommended for finetuning: 16 or 32) \n",
    "batch_size = 32\n",
    "\n",
    "#Focus: Training dataset\n",
    "#I. Create a train_data dataset from the tensors: train_inputs, train_labels and train_masks\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "#II. Create a sampler (iterator) that determines how batches are selected from the train_data (here: random)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "#III. Create batches using DataLoader\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#Print statement to understand the structure of train_dataloader\n",
    "print(\"DataLoader (train_dataloader):\")\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "#Focus: Validation dataset\n",
    "#I. Create a validation_data dataset from the tensors: validation_inputs, validation_labels and validation_masks\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "#II. Create a sampler (iterator) that determines how batches are selected from the validation_data (here: random)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "#III. Create batches using DataLoader\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XI. Approach 1: Initialize a BERT model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "#Create configuration object that defines architecture and hyperparameters of BERT model\n",
    "#This loads the default configuration for a BERT model\n",
    "configuration = BertConfig()\n",
    "\n",
    "#Initializing a model using the bert-base-uncased style configuration\n",
    "model = BertModel(configuration)\n",
    "\n",
    "#Accessing the model configuration\n",
    "configuration = model.config\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XII. Approach 2: Downloading a pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XIII. Group model parameters for L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model's parameters\n",
    "model_parameters = list(model.named_parameters())\n",
    "\n",
    "#Create no_decay: a list of two strings\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "grouped_model_parameters = [\n",
    "    #p refers to the parameter tensor, n refers to name of the parameter\n",
    "    {'params': [p for n, p in model_parameters if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.1},\n",
    "\n",
    "    {'params': [p for n, p in model_parameters if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XIV. Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I. Number of training epochs\n",
    "epochs = 4\n",
    "\n",
    "#II. Optimizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "#The AdamW optimizer is a variant of the Adam optimizer that includes weight decay (L2 regularization)\n",
    "#The weight decay is applied directly during the optimization step\n",
    "optimizer = AdamW(grouped_model_parameters,\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8\n",
    "                  )\n",
    "\n",
    "#III. Total number of training steps (number of batches * number of epochs)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "#IV. Learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XV. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize two empty lists\n",
    "t = []\n",
    "train_loss_set = []\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "  #I. Training\n",
    "\n",
    "  #Set model to training mode\n",
    "  model.train()\n",
    "\n",
    "  #Tracking variables\n",
    "  tr_loss = 0 #training loss\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0 #number of training examples/steps\n",
    "\n",
    "  #Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    #Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    #Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    #Clear out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs['loss']\n",
    "    train_loss_set.append(loss.item())\n",
    "\n",
    "    #Compute the gradients of the loss with respect to the model's parameters\n",
    "    loss.backward()\n",
    "\n",
    "    #Update model's parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    #Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    #Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0) #number of examples processed (accumulates the batches)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  #Calculate the average loss across the batches processed so far\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "  #II. Validation\n",
    "\n",
    "  #Set model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  #Tracking variables\n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  #Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    #Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    #Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    #Telling the model not to compute/store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      #Forward pass (no labels = logit predictions only)\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    #Move logits and labels to CPU\n",
    "    logits = outputs['logits'].detach().cpu().numpy() #Note to self: I believe we don't need detach() here as we used no_grad to calculate the outputs\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    #Calculate the accuracy of predictions vs labels\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "    \n",
    "  #Print the average accuracy over the batches processed so far\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XVI. Evaluate on Out-Of-Domain (OOD) test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df = pd.read_csv(\"out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II. Create sentences, label lists and add BERT tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "#Add [CLS] and [SEP] tokens at the beginning and end of each sentence for BERT\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "\n",
    "#Extract the labels from the DataFrame\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### III. Tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use tokenize()\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IV. Set max_len, convert tokens to IDs (index numbers in BERT vocabulary), and pad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set max_len\n",
    "MAX_LEN = 128\n",
    "\n",
    "#Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "#Pad the input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### V. Create attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "#Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VI. Convert data into torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VII. Select batch size and create iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select batch size\n",
    "batch_size = 32\n",
    "\n",
    "#I. Create a prediction_data from the tensors: prediction_inputs, prediction_masks and prediction_labels\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "\n",
    "#II. Create a sampler (iterator) that determines how batches are selected from prediction_data (here: random)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "\n",
    "#III. Create batches using DataLoader\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VIII. Define softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax logits\n",
    "import numpy as np\n",
    "\n",
    "def softmax(logits):\n",
    "    e = np.exp(logits)\n",
    "    return e / np.sum(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IX. Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#Tracking variables\n",
    "raw_predictions, predicted_classes, true_labels = [], [], []\n",
    "\n",
    "#Evaluate data for one epoch\n",
    "for batch in prediction_dataloader:\n",
    "  #Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "  #Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "  #Telling the model not to compute/store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    #Forward pass (no labels = logit predictions only)\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "  #Move logits, labels, and input_ids to CPU\n",
    "  logits = outputs['logits'].detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  b_input_ids = b_input_ids.to('cpu').numpy()\n",
    "  \n",
    "  #Convert input_ids back to words\n",
    "  batch_sentences = [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in b_input_ids]\n",
    "\n",
    "  #Apply softmax function to convert logits into probabilities\n",
    "  probabilities = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "\n",
    "  #The predicted class is the one with the highest probability\n",
    "  batch_predictions = np.argmax(probabilities, axis=1)\n",
    "\n",
    "  #Print the sentences and the corresponding predictions for this batch\n",
    "  for i, sentence in enumerate(batch_sentences):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Prediction: {logits[i]}\")\n",
    "    print(f\"Sofmax probabilities\", softmax(logits[i]))\n",
    "    print(f\"Prediction: {batch_predictions[i]}\")\n",
    "    print(f\"True label: {label_ids[i]}\")\n",
    "\n",
    "  #Store raw predictions, predicted classes and true labels\n",
    "  raw_predictions.append(logits)\n",
    "  predicted_classes.append(batch_predictions)\n",
    "  true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### X. Calculate Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I. Matthews Correlation Coefficient per batch\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "#Initialize an empty list\n",
    "matthews_set = []\n",
    "\n",
    "#Iterate over each batch\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  #Calculate Matthews correlation coefficient for each batch\n",
    "  matthews = matthews_corrcoef(true_labels[i], predicted_classes[i])\n",
    "\n",
    "  #Add the result to the matthews_set list\n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#II. Matthews Correlation Coefficient for the entire evaluation set\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "#Create one-dimensional list the true_labels and predicted_classes\n",
    "#List comprehension with nested loop (!):\n",
    "#[extracted_value for outer_loop_variable in outer_loop_iterable for inner_loop_variable in inner_loop_iterable]\n",
    "true_labels_total = [label for batch in true_labels for label in batch]\n",
    "predicted_classes_total = [pred for batch in predicted_classes for pred in batch]\n",
    "\n",
    "#Calculate the MCC for the entire set of predictions\n",
    "mcc = matthews_corrcoef(true_labels_total, predicted_classes_total)\n",
    "\n",
    "print(f\"MCC: {mcc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
